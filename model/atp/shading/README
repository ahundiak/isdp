INTRODUCTION

This set of Automatic Test Procedures (ATPs) is for the Modeling HSURF and 
VE commands.  ATPs for each command are implemented as ppl programs that test
various behaviors of the commands.  To verify correct command behavior, 
window data can be saved, in the form of "save image" rgb files
or data structures dumped by macro calls or debug messages, to be
compared with control values. Unix command scripts are available to 
sequence the test procedures and generate a status report for all
commands automatically.

TO RUN THE AUTOMATIC TEST PROCEDURES

run_atps [ [-f <filename>] [-d] [-h] ] [-s] [-v] [-n]

   -f filename : Read ATP and testfile locations from file <filename>.
   -h          : Show help screen.
   -d          : Execute default ATPs and testfile.
   -v          : Verbose.  This option will output differnces between the
                 generated output and the contol data. Optional.
   -s          : Save.  This option wil save screen dumps in the form of
                 rgb files. Optional.
   -n          : None.  Do not save or compare results.

Sample invocations:  run_atps -d -s -v  (run ATPs with default files, save and
                                         compare results)
                     run_atps -f test.file  (run ATPs,  getting atp and design
                                             file locations from test.file) 

Only one of the first options (-f, -d) is required to run the ATPs. The -n, 
-s and -v options are available for invocation from another script, or as a 
short cut for users. If the user does not select an -n, -v or -s option, on 
screen prompts will guide the user in selecting the proper options. 

If the default ATPs option switch is used in conjunction with an optional
switch, the default design file is automatically assumed.  If the default ATPs 
are to be run and the user does not select any of the optional switches, he 
will be asked if the default design file (shapes/shapes) is to be used.  If a 
different design file is desired, two things should be kept in mind:

         1) The design file specified should be a complete pathname.
         2) The design file will be used for all of the ATPs.

If either of these consequences are not desired, then the "-f filename" option
should be used.  The format of the file used to specify design file and ATP 
locations is:

     command directory        ATP pathname             Design File pathname
sample:     cw          /usr2/ingr/tests/TR1437.u  /usr2/ingr/test_file/TR1437 

A file "base.file" is provided in the atp/shading directoy that lists all the
current ATPs and the default design file.  This file can be used as is, or as
a prototype for users to follow in creating their own.


COMPONENTS OF THE TEST

Test Cases

These unit tests verify that each command works according to functional 
requirements from a user's perspective. For now, they are implemented 
using ppl as application-level journaling produces scripts that are 
easier to edit and less susceptible to slight application changes like 
window/button position shifts. At a later stage, more detailed journaling
at the toolkit or X protocol level can be incorporated.


Data Verifiers
	
To verify that the commands ran correctly without someone having to 
observe the results on the screen, data verifiers are used. 2 types of 
data verifiers are implemented: geometry verifiers and graphics display 
verifiers.

1. Geometry verifiers 
	
   Methods which compares instance data of objects are dloaded to verify 
   that the command has successfully modified the object database.

2. Graphics display verifiers
		
   Tools that compare raster files are used to verify that the geometry 
   has been displayed correctly. One such tool, "rle_view", differences 
   the pixel values of type 27 and type 29 raster files. It is used to 
   determine whether the display results matches the control within a
   given tolerance. If there are discrepancies, the image is saved so 
   that the differences can be viewed graphically later using "rle_view". 
   Since only images of the same size can be "diff"ed, control images for 
   each machine/screen type must be generated.  "rle_view" does not work 
   on the Sun yet because of X windows and big endian issues, so "sum" 
   is used instead.
	   

Test Case Sequencing

All the test cases are sequenced using Unix command scripts.




DIRECTORY STRUCTURE

                           $MODEL/atp/shading
                 ___________________|_______________________________
                |                   |         |         |           .	
        command directories       shapes     tmp     rle_view     README
      __________|____________       .         .         |        run_atps 
      .    .       |         |    shapes   atp.rpt   rle_view    proc_atp	
   README  .u     data	    tmp	 shapes.u	    rle_view.c 	gen_report
                   .         .                      read_file.c  atp.list 
                 .rgb      .rgb                     color_rgb.c  
                atp.log   atp.log                   rle_view.h
                          log.dif                   rle_view.m 

"run_atps" is the script which sequences the test procedures. It calls 
"proc_atp" which runs the test procedures, and "gen_report" which generates a 
status report if the control data files are available.  At this time, the 
command directories hold the default ATPs for that particular command.  The 
location of these ATPs may change in the future ( see below in future 
refinements ).


GETTING STARTED

cci - Make sure that ppl can access to omcpp, and include directories of 
      EXNUC, GRNUC, MODEL and BSPMATH.

shapes - Make sure that the design file shapes/shapes is present. It
         can be generated using shapes/shapes.u

rle_view - If running on the Clipper, make sure that rle_view/rle_view 
           has been generated (use rle_view.m)

control data files - To generate, execute "run_atps -d -s" or "run_atps  
           -f <filename> -s" once.  This will generate the files and place 
           them in the <command>/data directory. 
                     
To run "Create Shaded Plot File" atp's, the plot queue must be set up first. 



RESULT FORMAT

Ppl Macros

   Start "macro name"
   SUCCESS/FAILURE error_code (only if the status is FAILURE)
   Additional output if required 
   End "macro name"

Commands

   Start "command name"
   SUCCESS/FAILURE error_code (only if the status is FAILURE)
   Additional output if required 
   End "command name"

A separate set of error_codes can be defined for ATP's.



LIST OF COMMANDS TO BE TESTED

LEVEL 1

uw      Update Window
swdm    Set Window Display Mode
pdd     Process Detail Display
zoom    Zoom
cve     Create Visible Edges
cvd     Change View Dynamically	(Not implemented)
cct     Create Color Table
fv      Fit View
crw     Create Window
wa      Window Area
sctp    Set Color Table Parameters
di      Display Image (n.i.)
si      Save Image

LEVEL 2

mvp     Modify View Parameters 	
sdp     Set Display Parameters
cpa     Change Perspective Angle (n.i.)
pml     Place/Modify Light
cds     Change Detail Display
shs     Suppress Hidden Symbology (n.i.)
csa     Copy Shading Attributes (Snapshot of csa has a bug)
sasa    Set Active Shading Attributes
rvauv   Rotate View about Up Vector (n.i.)
wc      Window Center
sws     Set Window Shape
cw      Copy Window
dr      Display Region
scrv    Scroll View (n.i.)
cspf    Create Shaded Plot File	
sw      Swap Windows

LEVEL 3

rw      Rename Window
rvada   Rotate View about Defined Axis
rvb     Rotate View by 3 Points
av      Align Views (n.i.)
dy      Dynamics
tdia    Toggle Dynamics in All/Current
seazd   Set Active Z Depth
sedd    Set Display Depths
sevv    Set View Volume
mvv     Modify View Volume
wo      Window On; Window Off
dls     Define Line Style
rerfs   Retrieve Edge Results from Server (n.i.)
cpe     Create Projected Edges (n.i.)

LEVEL 4

cas     Cascade
max     Maximize; Restore
tile    Tile
vst     Verify Shaded Toolpath (n.i.)
sct     Save Color Table; Attach Color Table (n.i.)
ziv     Zoom In View (n.i.)
zov     Zoom Out View (n.i.)
cpptp   Convert Parallel Projection to Perspective \
        Convert Perspective Projection to Parallel
do      Delay On/off (n.i.)
ddd     Delete Detail Display (n.i.)
rvapbk  Rotate View about Point by Key In Values (n.i.)
rvaep   Rotate View about Eye Point (n.i.)
sazd    Show Active Z Depth (n.i.)
sdd     Show Display Depths (n.i.)
svv     Show View Volume (n.i.)
tilt    Tilt (n.i.)


FUTURE REFINEMENTS

ATP's are being developed in 2 phases:

 I. Black Box Tests

    Tests that verify that the software works according to functional 
    requirements from the user's perspective. Establishes input 
    conditions to detect: 

    - interface/display errors 
    - data structure errors 
    - incorrect/missing functions
    - performance errors

    The available ATP's checks for display and data structure errors, 
    but can be further refined as follows:

    STEP 1. Automatic non-interactive tests 
    
    1. Modify "rle_view" to compare and display rgb files on the Sun
       (big endian  and windowing issues).

    2. Refine the file structure:
   
       a. Divide the ATP's between $MODEL/shading, $MODEL/edges and 
          $GRNUC/window directory.

       b. Move all source files and directories into $MODEL/atp/src
          and the executables into $MODEL/atp/bin. 

    3. Develop ATP's for rest of the commands.
  
    4. Develop additional ppl programs to test more aspects of the 
       commands.

    5. Error codes in the status report generated.

    6. Refine snapshots for more detailed validation e.g. check that
       the user interface and Ems message strip messages are displayed
       correctly.

    7. Include boundary/performance tests e.g. C profiler, aprof 
       (memory monitor).


    STEP 2. Tools 

    1. Journaling on a lower-level (toolkit, X protocol).

    2. Debug capabilities

       a. Interrupting playback

       b. Break points (& eval e.g. by inserting action)

       c. Single step (& eval)

       d. Attach C debugger 

    3. Graphical user interface (like Xrecord)
	   
       a. Variable playback speed

       b. Insert action (ppl) 

       c. Insert shell scripts

       d. Insert comments
